{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOO97HYlapvgbT5Du0jMb8v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/romeshprasad/machine-learning-from-scratch/blob/main/simple_linear_regression_gradient_des_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from statistics import stdev #library for standardizing the data\n",
        "import pandas as pd #library for importing data\n",
        "import numpy as np #library for mathematical operation\n",
        "import matplotlib.pyplot as plt #library for plotting the data\n",
        "\n",
        "#load dataset\n",
        "data = pd.read_csv(\"Salary_Data.csv\")\n",
        "x = data.iloc[:,0]\n",
        "print(type(x))\n",
        "y = data.iloc[:,1]\n",
        "print(type(y))\n",
        "\n",
        "fig1, ax = plt.subplots(figsize =(10, 7))\n",
        "ax.hist(x)\n",
        "plt.show()\n",
        "\n",
        "fig2, ax = plt.subplots(figsize =(10, 7))\n",
        "ax.hist(y)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#parameter initialization\n",
        "theta_0 = 0\n",
        "theta_1 = 0\n",
        "learning_rate = 0.05\n",
        "N = x.shape[0]\n",
        "print(N)\n",
        "\n",
        "#building our own standard scalar\n",
        "#formual: z = (xi - mean)/standardeviation\n",
        "\n",
        "#mean_calculation\n",
        "mean_x = sum(x)/N\n",
        "print(mean_x)\n",
        "mean_y = sum(y)/N\n",
        "print(mean_y)\n",
        "\n",
        "\n",
        "\n",
        "#std_dev calculation\n",
        "std_dev_y = stdev(y)\n",
        "print(std_dev_y)\n",
        "std_dev_x = stdev(x)\n",
        "print(std_dev_x)\n",
        "\n",
        "#empty list for standardized x and y\n",
        "x_standardized = []\n",
        "y_standardized = []\n",
        "\n",
        "#iterating over all the x and y training samples and appending in the above empty list\n",
        "for i in x:\n",
        "    new_value_x = (i - mean_x)/std_dev_x\n",
        "    x_standardized.append(new_value_x)\n",
        "print(x_standardized)\n",
        "#print(type(x_standardized))\n",
        "for j in y:\n",
        "    new_value_y = (j - mean_y)/std_dev_y\n",
        "    y_standardized.append(new_value_y)\n",
        "print(y_standardized)\n",
        "#print(type(y_standardized))\n",
        "\n",
        "fig3, ax = plt.subplots(figsize =(10, 7))\n",
        "ax.hist(x_standardized)\n",
        "plt.show()\n",
        "\n",
        "fig4, ax = plt.subplots(figsize =(10, 7))\n",
        "ax.hist(y_standardized)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "#gradient_descent_function: minimize(theta)j = theta(j) - learning_rate*(part_der w.r.t.(theta)j * lossfunction)\n",
        "def gradient_descent(x_standardized,y_standardized,theta_0, theta_1,learning_rate):\n",
        "\tpdtheta_0 = 0.0\n",
        "\tpdtheta_1 = 0.0\n",
        "\t#loop through x and y\n",
        "\t#since X0 = 1, we wont write in our equation\n",
        "\tfor xi, yi in zip(x_standardized,y_standardized): \n",
        "\t\t#this will loop over all the training example: N\n",
        "\t\t#first calculating partial derivative for x0, which will give us theta_0\n",
        "\t\tpdtheta_0 += ((theta_0 + theta_1*xi)-yi)*1\n",
        "\t\t#next calculating partial derivative for x1, which will give us theta_1\n",
        "\t\tpdtheta_1 += ((theta_0 + theta_1*xi)-yi)*xi\n",
        "\ttheta_0 = theta_0 - learning_rate * pdtheta_0\n",
        "\ttheta_1 = theta_1 - learning_rate* pdtheta_1\n",
        "\treturn theta_0 , theta_1\n",
        "\n",
        "#Running gradient descent\n",
        "average_loss = []\n",
        "for epoch in range(200):\n",
        "\ttheta_0 , theta_1 = gradient_descent(x_standardized,y_standardized, theta_0, theta_1, learning_rate)\n",
        "\ty_pred = theta_0 + theta_1*np.asarray(x_standardized)\n",
        "\tprint(y_pred) \n",
        "\tloss = np.divide(np.sum((y_standardized - y_pred)**2), N)\n",
        "\tprint(loss)\n",
        "\taverage_loss.append(loss)\n",
        "\tprint(f'{epoch} loss is {loss}, parameters theta[0] : {theta_0}, theta[1] :{theta_1}')\n",
        "\n",
        "\n",
        "#visualizing the data\n",
        "plt.plot(average_loss)\n",
        "#plt1=  plt.ylabel(\"Per Capita Income\")\n",
        "#plt1 = plt.xlabel(\"Year\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qAo9ZMVvrqhi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}